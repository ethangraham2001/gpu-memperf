# GPUMemperf

Memory bandwidth is the primary bottleneck for scaling GPU-accelerated
applications, yet coarse-grained benchmarks often obscure the nuances of the
memory hierarchy. GPUMemperf is a high-precision benchmarking suite for NVIDIA
architectures designed to isolate performance across L1/L2 caches,
shared memory, and global memory. By leveraging PTX-level cache control and
on-GPU pointer chasing, our framework characterizes hardware limits under
varied strided and random access patterns. Furthermore, we evaluate the impact
of asynchronous data movement on register pressure and compute-bound throughput.
GPUMemperf provides an extensible, developer-centric toolset for identifying
the architectural "cliffs" that govern performance in modern HPC workloads.

## Supported Benchmarks

GPUMemperf implements a set of benchmarks designed to stress the memory
subsystem at all levels of the hierarchy under realistic conditions. The
exhaustive list is found below.

### Global to Shared Memory Transfers (Synchronous vs. Asynchronous)

This benchmark measures the performance of naive cooperative loading of data
from global memory (DRAM) into shared memory versus a double-buffered
asynchronous implementation leveraging `cuda_memcpy_async`. A tunable amount of
work is performed for each data element, simulating a realistic environment
where the benchmark is either compute-bound or memory bound-depending on the
configuration.

The key insight provided by this benchmark is being able to figure out how much
work needs to be done for it to be worth using asynchronous DMA copies over
naive copying, i.e., when does the added synchronization overhead become
amortized by reducing register pressure and being able to interleave computation
with data movement.

### Strided Access

This benchmark loads data from either cache or memory (configurable) in a linear
pattern. The unit-stride configuration (where `stride=1`) should be very close
to the maximum achievable bandwidth - our experiments on the RTX 5060Ti card
showed that the measured DRAM bandwidth was within 10% of its advertised
bandwidth of 448GB/s.

The key insight provided by this benchmark is being able to measure the maximum
throughput that is realistically achievable on a given GPU at all levels of the
cache/memory hierarchy at different amounts of striding.


### Shared Memory to Registers

This benchmark measures the bandwidth of linear shared-memory-to-register
transfers with different configurable strides.

The key insight provided by this benchmark is quantifying the performance
degradation incurred by bank conflicts during shared memory accesses.

### Random Access

This benchmark loads data from cache/memory in a random pattern generated by the
host.

The insight provided by this benchmark is the worst-case performance of a memory
bound kernel, as GPUs are highly optimized for predictable memory access
patterns.

### Pointer Chase

This benchmark, in contrast with the others, is a latency measurement benchmark.
The host builds a "pointer-chain" which is iterated by the GPU with the
following relation:

```
idx = 0
for i in 0...iters:
    idx = indices[idx]
```

Since each iteration has a serial dependency on the previous iteration, the
total runtime divided by the number of accesses is roughly equal to the average
access latency.

Performing a sweep over varying working set sizes allows us to identify when
we have overflowed the current cache level - such points are marked with sudden
latency increases.

The key insight provided by this benchmark is being able to automatically
determine the **detectable** cache sizes and latencies for each level in the
memory hierarchy.

## Building The `gpu-memperf` Binary

This project requires the NVIDIA toolchain. It is recommended to have a
relatively up-to-date version of this. Ubuntu instructions can be found on
[this page](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/). On
Arch, it can be installed like so:

```sh
pacman -S cuda
```

Then, just run `make` (or `make all`) to build the binary.

Python dependencies for the Orchestrator script can be installed like so

```bash
pip install -r python/requirements.txt
```

## Usage

There are two ways to run collect GPU bandwidth metrics using GPUMemperf.

1. Using the Orchestrator script, under `python/orchestrator.py` (recommended).
2. Running the `gpu-memperf` binary directly.

### 1. The Orchestrator Script

The Orchestrator is a wrapper around the core `gpu-memperf` binary that handles
calling it with custom parameters, launching multiple benchmarks, and doing any
data post-processing and plotting.

Before running the script, ensure that all dependencies in
`python/requirements.txt` are installed.

The Orchestrator, in `python/orchestrator.py` contains three distinct modes.

1. Cache-size finder: runs an on-GPU pointer-chase algorithm to determine the
   capacity and latency of L1, L2, and DRAM.
2. Properties finder, which outputs a `.json` file containing a reduced set of
   device-specific attributes and some additional metadata.
3. Orchestrator, that runs custom benchmarks on the hardware.


#### Cache Size Finder

To run this, simply launch the script like so:

```bash
python python/orchestrator.py cache-sizes
```

The benchmark will take some time to spawn, before outputting the cache
properties as JSON into the command line.

#### Properties Finder

The properties finder mode will run a set of benchmarks on the GPU to collect
certain physical properties and dump them into a JSON file. This mode takes one
argument `out` which specifies the location of the `.json` output file.

```bash
python python/orchestrator.py properties --out properties.json
```

We provide a sample output for this program:

```json
{
    "system_info": {
        "commit": "c8f1d085b1c70189d28ca12631c7c49a33d27925"
    },
    "linear_bandwidth": {
        "l1_cache": {
            "max": 7620610000000.0,
            "min": 7607270000000.0,
            "mean": 7612796666666.667,
            "median": 7610510000000.0
        },
        "l2_cache": {
            "max": 2124230000000.0,
            "min": 2122650000000.0,
            "mean": 2123240000000.0,
            "median": 2122840000000.0
        },
        "dram": {
            "max": 428018000000.0,
            "min": 427958000000.0,
            "mean": 427992666666.6667,
            "median": 428002000000.0
        }
    },
    "estimated_cache_properties": {
        "L1": {
            "capacity": "92160",
            "latency": 47.5627
        },
        "L2": {
            "capacity": "31981568",
            "latency": 345.478
        },
        "DRAM": {
            "capacity": "N/A",
            "latency": 786.992
        }
    }
}
```

#### Orchestrator

The main Orchestrator is capable of running a more comprehensive set of
benchmarks which are specified in the command line, and handles plot generation.
This is recommended for users that want more granular (and visual) into the
behavior and performance of their GPUs memory system.

The supported benchmarks are:

- Global to Shared memory (`global_to_shared`).
- Random Access for L1 Cache (`random_access_l1`).
- Random Access for L2 Cache (`random_access_l2`).
- Random Access for DRAM (`random_access_dram`).
- Strided Access for L1 Cache (`strided_l1`).
- Strided Access for L2 Cache (`strided_l2`).
- Strided Access for DRAM (`strided_dram`).
- Shared Memory to Registers (`shared_to_registers`).
- Pointer Chase on GPU for Detecting Cache Size (`pchase_gpu`).

The program takes three (optional) arguments:

- The number of repetitions per experiment (`--reps`).
- The output directory (`--out`).
- A list of programs to run (each prefixed with `--program`). Defaults to all
  programs listed above.

For example, the Global to Shared memory and L1 Random Access benchmarks could
be launched like so:

```bash
python python/orchestrator.py orchestrator  \
    --program=global_to_shared              \
    --program=random_access_l1              \
    --out=custom-directory
```

### The `gpu-memperf` Binary

```bash
./gpu-memperf <benchname> <list-of-arguments>
```

For example, for the Global to Shared memory benchmark can be launched like so:

```sh
./gpu-memperf global_to_shared  \
    --flops_per_elem=2,4,8      \
    --threads_per_block=1024    \
    --num_blocks=108
```

The full set of options are defined by the individual benchmarks, which is why
it's easier to use the Orchestrator that wraps all of this logic and is easier
to tinker with without needing to explore the C++ source code.

All benchmarks are implemented to emit `.csv` files, and occasionally additional
plaintext files, to be consumed during data post-processing. All emitted files
are output into a timestamped directory under `./logs`.

## Report Bugs

If something doesn't work, please create an issue.
